{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataStream Class\n",
    "\n",
    "The `DataStream` class is a holder and merger of data timeseries and metadata. It is the building block of the `DataLake` class which is what you will mainly be interested in. This walkthrough will show you how to use the `DataStream` class to load data from a file and calculate time averages.\n",
    "\n",
    "\n",
    "```python\n",
    "class DataStream():\n",
    "    \n",
    "    \"\"\"A class for calculating time averages of data streams on-the-fly.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        header_list (list of str): A list of column headers for the data.\n",
    "        average_times (list of float): A list of time intervals (in seconds)\n",
    "            over which to calculate the data averages.\n",
    "        average_base (float, optional): The base for the exponential moving\n",
    "            average window used to smooth the data. Defaults to 2.0.\n",
    "\n",
    "    Example:\n",
    "    ----------\n",
    "        # Create a DataStream object with two headers and two average intervals\n",
    "        stream = DataStream(['time', 'value'], [10.0, 60.0])\n",
    "\n",
    "        # Add data to the stream and calculate the rolling averages\n",
    "        for data_point in my_data_stream:\n",
    "            stream.add_data_point(data_point)\n",
    "            rolling_averages = stream.calculate_averages()\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "        header_list (list of str): (n,) A list of column headers for the data.\n",
    "        header_dict (dict): A dictionary mapping column headers to column\n",
    "            indices.\n",
    "        average_int_sec (list of float): A list of time intervals (in seconds)\n",
    "            over which to calculate the data averages.\n",
    "        data_stream (np.ndarray): (n, m) An array of data points.\n",
    "            Rows match header indexes and columns match time.\n",
    "        time_stream (np.ndarray): (m,) An array of timestamps corresponding\n",
    "            to the data points.\n",
    "        average_base_sec (float): The base for the exponential moving average\n",
    "            window used to smooth the data.\n",
    "        average_base_time (np.ndarray): An array of timestamps corresponding to\n",
    "            the beginning of each average interval.\n",
    "        average_base_data (np.ndarray): An array of average data values for\n",
    "            each average interval.\n",
    "        average_base_data_std (np.ndarray): An array of standard deviations of\n",
    "            the data values for each average interval.\n",
    "        average_epoch_start (int or None): The Unix epoch time (in seconds) of\n",
    "            the start of the first average interval.\n",
    "        average_epoch_end (int or None): The Unix epoch time (in seconds) of\n",
    "            the end of the last average interval.\n",
    "        average_dict (dict): A dictionary mapping average interval lengths to\n",
    "            corresponding arrays of average data values.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, dates\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from datacula.lake.datastream import DataStream\n",
    "from datacula import loader\n",
    "\n",
    "# preferred settings for plotting\n",
    "plt.rcParams.update({'text.color': \"#333333\",\n",
    "                     'axes.labelcolor': \"#333333\",\n",
    "                     \"figure.figsize\": (6,4),\n",
    "                     \"font.size\": 14,\n",
    "                     \"axes.edgecolor\": \"#333333\",\n",
    "                     \"axes.labelcolor\": \"#333333\",\n",
    "                     \"xtick.color\": \"#333333\",\n",
    "                     \"ytick.color\": \"#333333\",\n",
    "                     \"pdf.fonttype\": 42,\n",
    "                     \"ps.fonttype\": 42})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings dictionary\n",
    "\n",
    "This dictionary, `settings_cpc`, contains configuration settings for a specific instrument called \"CPC_3010\". The settings are used for data processing, loading, and organization. Let's go through each key-value pair in the dictionary:\n",
    "\n",
    "- \"instrument_name\": \"CPC_3010\": The name of the instrument, which is \"CPC_3010\" in this case.\n",
    "- \"data_stream_name\": \"CPC_3010\": The name of the data stream associated with the instrument, also \"CPC_3010\" here.\n",
    "- \"data_loading_function\": \"general_load\": The name of the function that will be used to load the data. In this case, it's a general loading function called \"general_load\".\n",
    "- \"relative_data_folder\": \"CPC_3010_data\": The name of the folder where the data files for this instrument are stored, relative to the main data directory.\n",
    "- \"skipRowsDict\": 0: The number of rows to skip at the beginning of the file while reading the data. In this case, no rows are skipped.\n",
    "- \"Time_shift_to_Linux_Epoch_sec\": 0: The time shift (in seconds) to convert the timestamps in the data to Unix/Linux epoch time. No time shift is applied here.\n",
    "- \"data_checks\": This dictionary contains data validation settings:\n",
    "- \"characters\": [20,35]: The range of valid line lengths (in characters) for each row in the data file.\n",
    "- \"char_counts\": {\",\": 4, \"/\": 0, \":\": 0}: The expected number of occurrences of specific characters in each row of the data file. In this case, 4 commas are expected, while no forward slashes or colons should be present. This is general, more character-specific checks can be added to the dictionary.\n",
    "- \"skip_rows\": 1: The number of rows to skip at the beginning of the file for data validation.\n",
    "- \"skip_end\": 0: The number of rows to skip at the end of the file for data validation.\n",
    "- \"data_header\": [\"CPC_count[#/sec]\", \"Temp_[C]\"]: The list of data column headers in the file.\n",
    "- \"data_column\": [1, 2]: The indices of the data columns in the file.\n",
    "- \"time_column\": 0: The index of the time column in the file.\n",
    "- \"time_format\": \"epoch\": The format of the time values in the file. In this case, it's Unix/Linux epoch time.\n",
    "- \"filename_regex\": \"CPC_3010*.csv\": The regular expression pattern to match the data file names for this instrument.\n",
    "- \"base_interval_sec\": 2: The base time interval (in seconds) for the data.\n",
    "- \"data_delimiter\": \",\": The delimiter used to separate the data values in the file, which is a comma in this case.\n",
    "\n",
    "The `settings_cpc` dictionary is a useful way to store all the necessary settings and configurations for a particular instrument or data stream. These settings can be used by various functions and methods to process and analyze the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_cpc = {\n",
    "        \"instrument_name\": \"CPC_3010\",\n",
    "        \"data_stream_name\": \"CPC_3010\",\n",
    "        \"data_loading_function\": \"general_load\",\n",
    "        \"relative_data_folder\": \"CPC_3010_data\",\n",
    "        \"skipRowsDict\": 0,\n",
    "        \"Time_shift_to_Linux_Epoch_sec\": 0,\n",
    "        \"data_checks\": {\n",
    "            \"characters\": [20,35],\n",
    "            \"char_counts\": {\",\": 4, \"/\": 0, \":\": 0},\n",
    "            \"skip_rows\": 1,\n",
    "            \"skip_end\": 0\n",
    "        },\n",
    "        \"data_header\": [\"CPC_count[#/sec]\", \"Temp_[C]\"],\n",
    "        \"data_column\": [1,2],\n",
    "        \"time_column\": 0,\n",
    "        \"time_format\": \"epoch\",\n",
    "        \"filename_regex\": \"CPC_3010*.csv\",\n",
    "        \"base_interval_sec\": 2,\n",
    "        \"data_delimiter\": \",\"\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path\n",
    "\n",
    "Set the working path where the data is stored. For now we'll use the provided example data in this current directory.\n",
    "\n",
    "But the path could be any where on your computer. For example, if you have a folder called \"data\" in your home directory, you could set the path to: `path = \"U:\\\\data\\\\processing\\\\Campgain2023_of_aswsome\\\\data\\\\\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parent directory of the data folder, for now this is the same as the current working directory\n",
    "# but this can be a completely different path\n",
    "\n",
    "path = os.getcwd()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "With the working directory set, we can now load the data. For this we use the `loader` module and call loader.data_raw_loader() with the file path as argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file = os.path.join(path, 'data', settings_cpc['relative_data_folder'], 'CPC_3010_data_20220701_Jul.csv')\n",
    "\n",
    "print(data_file)\n",
    "\n",
    "# load the data\n",
    "raw_data = loader.data_raw_loader(data_file)\n",
    "\n",
    "# print the first 2 rows\n",
    "print(raw_data[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data\n",
    "\n",
    "Now we can apply some data checks that we defined in the settings dictionary. For this we use `loader.data_format_checks` and then we can convert that list of strings to a numpy array.\n",
    "\n",
    "To do that next step we call `loader.sample_data()` with inputs from the settings dictionary and the data list we just cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is done by the general_data_formatter function for timeseries data\n",
    "# 2d data is a separate function\n",
    "\n",
    "data = loader.data_format_checks(raw_data, settings_cpc['data_checks'])\n",
    "\n",
    "# Sample the data to get the epoch times and the data\n",
    "epoch_time, data_array = loader.sample_data(\n",
    "    data=data,\n",
    "    time_column=settings_cpc['time_column'],\n",
    "    time_format=settings_cpc['time_format'],\n",
    "    data_columns=settings_cpc['data_column'],\n",
    "    delimiter=settings_cpc['data_delimiter'],\n",
    "    date_offset=None,\n",
    "    seconds_shift=settings_cpc['Time_shift_to_Linux_Epoch_sec'],\n",
    ")\n",
    "\n",
    "print(f\"epoch_time: {epoch_time.shape}\")\n",
    "print(epoch_time[:5])\n",
    "print(f\"data_array: {data_array.shape}\")\n",
    "print(data_array[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Step Loading\n",
    "\n",
    "We can also do all of the above steps in one step using the `loader.data_loader()` function. This function combines the two calls above into one. It the raw data directory and the settings dictionary as arguments. It then loads the data, applies the data checks, and converts the data to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_time, data = loader.general_data_formatter(\n",
    "    data=raw_data,\n",
    "    data_checks=settings_cpc['data_checks'],\n",
    "    data_column=settings_cpc['data_column'],\n",
    "    time_column=settings_cpc['time_column'],\n",
    "    time_format=settings_cpc['time_format'],\n",
    "    delimiter=settings_cpc['data_delimiter'],\n",
    "    date_offset=None,\n",
    "    seconds_shift=settings_cpc['Time_shift_to_Linux_Epoch_sec']\n",
    ")\n",
    "\n",
    "# Transpose the data\n",
    "# data = data.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataStream\n",
    "\n",
    "Now we can create a DataStream object with the data we just loaded. We'll use the `DataStream` class from the `datastream` module. We'll use the `header_list` and `average_times` from the settings dictionary to initialize the DataStream object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the datastream object\n",
    "cpc_datastream = DataStream(\n",
    "                    header_list=settings_cpc['data_header'],\n",
    "                    average_times=[600],\n",
    "                    average_base=settings_cpc['base_interval_sec']\n",
    "                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the DataStream\n",
    "\n",
    "Now we can fill the DataStream object with the data we just loaded. We'll use the `DataStream.add_data_point()` method to add each data point to the DataStream object.\n",
    "\n",
    "Note the tranpose of the data, we are expecting rows to be the different measurements and columns to be the different time points. So we need to transpose the data before adding it to the DataStream object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first transpose the data\n",
    "data = data.T\n",
    "\n",
    "cpc_datastream.add_data(\n",
    "            time_stream=epoch_time,\n",
    "            data_stream=data,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is in there\n",
    "\n",
    "The datastream object has a lot of attributes and methods. We can see them all by using the `dir()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we have in the datastream?\n",
    "dir(cpc_datastream)[27:]  # the first 27 are the default python methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "\n",
    "Now with the data in the DataStream object we can plot it. We can use the return_* methods to get the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]'])[0],\n",
    "    label='CPC data'\n",
    ")\n",
    "plt.tick_params(rotation=-35)\n",
    "ax.set_ylabel('CPC_count[#/sec]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d %Hhr'))\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the raw data\n",
    "\n",
    "If you want the raw unaveraged data add raw=True to the return_* methods. The default is raw=False, and you get the averaged data, which is on a uniform time grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=False, raw=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]'], raw=True)[0],\n",
    "    label='CPC data'\n",
    ")\n",
    "plt.tick_params(rotation=-35)\n",
    "ax.set_ylabel('CPC_count[#/sec]')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaverage the data\n",
    "\n",
    "We can also reaverage the data. We can use the `DataStream.reaverage()` method to reaverage the data. This method takes the `reaverage_base_sec` from the settings dictionary as an argument, or you can specify a different base time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_long = 300  # 5 min\n",
    "\n",
    "cpc_datastream.reaverage(reaverage_base_sec=average_long)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]'])[0],\n",
    "    label='CPC data'\n",
    ")\n",
    "plt.tick_params(rotation=-35)\n",
    "ax.set_ylabel('CPC_count[#/sec]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d %Hhr'))\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Ranges\n",
    "\n",
    "You can also specify the time range to average over. This is useful if you want to average over a specific time range. For example, if you want to average over the first 10 minutes of data, you can specify the start and end times as arguments to the `DataStream.reaverage()` method.\n",
    "\n",
    "Note there is some timezone vs UTC (epoch) you may need to shift around. Automating this is on the to-do list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.fromisoformat('2022-07-01T11:00:00').timestamp()-6*3600 # UTC shift\n",
    "end = datetime.fromisoformat('2022-07-01T16:00:00').timestamp()-6*3600 # UTC shift\n",
    "\n",
    "average_long = 600  # 5 min\n",
    "\n",
    "cpc_datastream.reaverage(\n",
    "    reaverage_base_sec=average_long,\n",
    "    epoch_start=start,\n",
    "    epoch_end=end\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]'])[0],\n",
    "    label='CPC data'\n",
    ")\n",
    "plt.tick_params(rotation=-35)\n",
    "ax.set_ylabel('CPC_count[#/sec]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d %Hhr'))\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaverage and Raw Data\n",
    "\n",
    "The raw data is always stored in the DataStream object. So if you want to reaverage the raw data, you can use the `DataStream.reaverage()` method to set a new interval, and set epoch_start and epoch_end to None to average over the entire data range again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_long = 300  # 5 min\n",
    "\n",
    "cpc_datastream.average_epoch_start = None\n",
    "cpc_datastream.average_epoch_end = None\n",
    "\n",
    "cpc_datastream.reaverage(\n",
    "    reaverage_base_sec=average_long,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]'])[0],\n",
    "    label='CPC data'\n",
    ")\n",
    "plt.tick_params(rotation=-35)\n",
    "ax.set_ylabel('CPC_count[#/sec]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d %Hhr'))\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more data\n",
    "\n",
    "As the `DataStream.add_data()` would imply, you can add more data to the DataStream object. So to do that lest first load some more data.\n",
    "\n",
    "This process does usually assume you adding more data that has the same headers, with an excpetion for sizer data. But adding the data is key, you can not rewrite the data, but you can add more data. There are not checks for same time stamps, so you can add the same data twice, but you will have duplicate data points.\n",
    "\n",
    "Note the time stamps are sorted when coming in, but not sorted again after the data is appended."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data July 9th added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file = os.path.join(path, 'data', settings_cpc['relative_data_folder'], 'CPC_3010_data_20220709_Jul.csv')\n",
    "\n",
    "print(data_file)\n",
    "\n",
    "# load the data\n",
    "raw_data = loader.data_raw_loader(data_file)\n",
    "\n",
    "\n",
    "epoch_time, data2 = loader.general_data_formatter(\n",
    "    data=raw_data,\n",
    "    data_checks=settings_cpc['data_checks'],\n",
    "    data_column=settings_cpc['data_column'],\n",
    "    time_column=settings_cpc['time_column'],\n",
    "    time_format=settings_cpc['time_format'],\n",
    "    delimiter=settings_cpc['data_delimiter'],\n",
    "    date_offset=None,\n",
    "    seconds_shift=settings_cpc['Time_shift_to_Linux_Epoch_sec']\n",
    ")\n",
    "\n",
    "# first transpose the data\n",
    "data2 = data2.T\n",
    "\n",
    "cpc_datastream.add_data(\n",
    "            time_stream=epoch_time,\n",
    "            data_stream=data2,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data July 10th added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(path, 'data', settings_cpc['relative_data_folder'], 'CPC_3010_data_20220710_Jul.csv')\n",
    "\n",
    "print(data_file)\n",
    "\n",
    "# load the data\n",
    "raw_data = loader.data_raw_loader(data_file)\n",
    "\n",
    "\n",
    "epoch_time, data2 = loader.general_data_formatter(\n",
    "    data=raw_data,\n",
    "    data_checks=settings_cpc['data_checks'],\n",
    "    data_column=settings_cpc['data_column'],\n",
    "    time_column=settings_cpc['time_column'],\n",
    "    time_format=settings_cpc['time_format'],\n",
    "    delimiter=settings_cpc['data_delimiter'],\n",
    "    date_offset=None,\n",
    "    seconds_shift=settings_cpc['Time_shift_to_Linux_Epoch_sec']\n",
    ")\n",
    "\n",
    "# first transpose the data\n",
    "data2 = data2.T\n",
    "\n",
    "cpc_datastream.add_data(\n",
    "            time_stream=epoch_time,\n",
    "            data_stream=data2,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Plot\n",
    "\n",
    "Now with the new data added we can update the plot. We can use the `DataStream.return_*` methods to get the data we want. The adding of data does not automatically reaverage the data, so we need to call the `DataStream.reaverage()` method to reaverage the data, to include the new data.\n",
    "\n",
    "The data covers July 1st, 9th, and 10th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_datastream.reaverage(\n",
    "    reaverage_base_sec=600,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]'])[0],\n",
    "    label='CPC data'\n",
    ")\n",
    "plt.tick_params(rotation=-35)\n",
    "ax.set_ylabel('CPC_count[#/sec]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d day'))\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data derived Products\n",
    "\n",
    "Lets say you made some calculations on the data, and you want to add those to the DataStream object. You can do that with the `DataStream.add_processed_data()` method. This method takes the name of the derived product, the data, and the units as arguments.\n",
    "\n",
    "Here, I pulled out the raw and averaged data, and mulitplied them by 5 and 10. We'll see how the get added to the DataStream object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mulitple = cpc_datastream.return_data(keys=['CPC_count[#/sec]'], raw=True)[0]*5\n",
    "raw_time = cpc_datastream.return_time(raw=True)\n",
    "\n",
    "\n",
    "avg_mulitple = cpc_datastream.return_data(keys=['CPC_count[#/sec]'], raw=True)[0]*10\n",
    "avg_time = cpc_datastream.return_time()\n",
    "\n",
    "print(f\"Raw length: {len(raw_time)}, Avg length: {len(avg_time)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add_processed_data\n",
    "\n",
    "Now we'll add these derived products to the DataStream object. We'll use the `DataStream.add_processed_data()` method to add the data to the DataStream object.\n",
    "\n",
    "Given the different lengths of the data, add_processed_data will interpolate the averaged data to the raw timestamps and then add the data to the datastream object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_datastream.add_processed_data(\n",
    "                data_new=raw_mulitple,\n",
    "                time_new=raw_time,\n",
    "                header_new=['CPC_count[#/sec]_rawMultiply'],\n",
    "            )\n",
    "\n",
    "cpc_datastream.add_processed_data(\n",
    "                data_new=avg_mulitple,\n",
    "                time_new=avg_time,\n",
    "                header_new=['CPC_count[#/sec]_avgMultiply'],\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the derived products\n",
    "\n",
    "Now we can reaverage the data and plot the derived products. We'll use the `DataStream.reaverage()` method to reaverage the data, and the `DataStream.return_*` methods to get the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_datastream.reaverage(\n",
    "    reaverage_base_sec=600,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]'])[0],\n",
    "    label='CPC data'\n",
    ")\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]_rawMultiply'])[0],\n",
    "    label='CPC data raw multiply'\n",
    ")\n",
    "ax.plot(\n",
    "    cpc_datastream.return_time(datetime64=True),\n",
    "    cpc_datastream.return_data(keys=['CPC_count[#/sec]_avgMultiply'])[0],\n",
    "    label='CPC data avg multiply'\n",
    ")\n",
    "plt.tick_params(rotation=-35)\n",
    "ax.set_ylabel('CPC_count[#/sec]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d day'))\n",
    "ax.legend()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have now loaded data, added it to a DataStream object, and added derived products to the DataStream object. There are additional methods that one can use, like return_std for the standard deviation.\n",
    "\n",
    "Next we'll be looking at another layer of abstraction, the pooling of these streams into a `DataLake` object. This will allow us to pool data from multiple instruments and data streams into a single object. This will allow us to do things like compare data from different instruments, or compare data from the same instrument at different locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacula_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
