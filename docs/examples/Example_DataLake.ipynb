{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLake Class\n",
    "\n",
    "The `DataLake` class is a holder and merger of multiple `DataStreams`. This will be the main class that you will be using to load data and perform operations on it.\n",
    "\n",
    "\n",
    "```python\n",
    "class DataLake():\n",
    "    \"\"\"\n",
    "    DataLake class to store and manage datastreams.\n",
    "\n",
    "    This class provides a simple way to manage multiple datastreams, where\n",
    "    each datastream is stored as a separate instance of the DataStream class.\n",
    "    Datastreams can be added to the DataLake using the add_data method, and\n",
    "    can be accessed or modified using the various methods provided by the\n",
    "    DataStream class.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "        settings (dict): A dictionary of settings for each datastream to be\n",
    "            added to the DataLake. The keys of the dictionary should be the\n",
    "            names of the datastreams, and the values should be dictionaries\n",
    "            containing the settings for each datastream.\n",
    "        path (str): The path to the data to be read in. Default is None.\n",
    "        utc_to_local (int): The UTC to local time offset in hours.\n",
    "            Default is 0.\n",
    "\n",
    "    Methods:\n",
    "    ----------\n",
    "        list_datastream: Returns a list of the datastreams in the DataLake.\n",
    "        update_datastream: Updates all datastreams in the DataLake.\n",
    "        add_processed_datastream: Adds a processed datastream to the DataLake.\n",
    "        initialise_datastream: Initialises a datastream using the settings in\n",
    "            the DataLake object.\n",
    "        reaverage_datastream: Reaverages the data in the specified datastream.\n",
    "        remove_zeros: Removes filter/zeros from the specified datastream.\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, dates\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from datacula.lake.datalake import DataLake\n",
    "\n",
    "\n",
    "# preferred settings for plotting\n",
    "plt.rcParams.update({'text.color': \"#333333\",\n",
    "                     'axes.labelcolor': \"#333333\",\n",
    "                     \"figure.figsize\": (6,4),\n",
    "                     \"font.size\": 14,\n",
    "                     \"axes.edgecolor\": \"#333333\",\n",
    "                     \"axes.labelcolor\": \"#333333\",\n",
    "                     \"xtick.color\": \"#333333\",\n",
    "                     \"ytick.color\": \"#333333\",\n",
    "                     \"pdf.fonttype\": 42,\n",
    "                     \"ps.fonttype\": 42})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLake Settings\n",
    "\n",
    "The initial settings dictionary holds all dataformat information, so it can get a little complex. We show and example here for the APS_data, CAPS_dual_data, CPC_3010_data, PASS3_data and SMPS_data files. This should give you an idea of how to set up the settings dictionary for your own data.\n",
    "\n",
    "You settings could be saved as a JSON file and loaded in, but we will just set them up manually here.\n",
    "\n",
    "The general format is:\n",
    "\n",
    "```python\n",
    "\n",
    "\"instrument_name\": str  # the name of the instrument that generated the data (in this case, \"PASS3\").\n",
    "\"data_stream_name\": str  # a short name for the data stream.\n",
    "\"data_processing_function\": str  # the name of a Python function that should be used to process the data (not specified in this case).\n",
    "\"data_loading_function\": str  # the name of a Python function that should be used to load the data.\n",
    "\"relative_data_folder\": str  # the name of the folder where the data is stored relative to the project directory.\n",
    "\"skipRowsDict\": int  # the number of rows to skip when loading the data (not specified in this case).\n",
    "\"Time_shift_to_Linux_Epoch_sec\": int  # the time shift in seconds to convert to Linux Epoch time (not specified in this case).\n",
    "\"data_checks\": dic[str]  # a dictionary that specifies various checks to perform on the data, such as the expected number of characters and delimiters.\n",
    "\"data_header\": list[str]  # a list of column headers for the data.\n",
    "\"data_column\": list[int]  # a list of column indices for the data.\n",
    "\"time_column\": list[int]  # a list of column indices that specify the time data.\n",
    "\"time_format\": str  # the format of the time data.\n",
    "\"filename_regex\": str  #a regular expression that matches the filenames of the data files.\n",
    "\"base_interval_sec\": int  # the time interval in seconds for the data.\n",
    "\"data_delimiter\": str  # the delimiter used in the data file.\n",
    "\n",
    "```\n",
    "\n",
    "There is a difference for 2d data, like APS and SMPS, where the data is stored in a 2d array. In this case, the data_header and data_column are lists of lists, where each list contains the headers and column indices for each 2d array. It produces two data streams, one for the 1d data and one for the 2d data.\n",
    "\n",
    "```python\n",
    "\n",
    "\"instrument_name\": str  # the name of the instrument that generated the data (in this case, \"SMPS_data\").\n",
    "\"data_stream_name\": str  # a list of two names for the data streams.\n",
    "\"data_processing_function\": str  # the name of a Python function that should be used to process the data (\"SMPS_processing\").\n",
    "\"data_loading_function\": str  #the name of a Python function that should be used to load the data (\"general_2d_sizer_load\").\n",
    "\"relative_data_folder\": str  #the name of the folder where the data is stored relative to the project directory.\n",
    "\"skipRowsDict\": int  #the number of rows to skip when loading the data (not specified in this case).\n",
    "\"Time_shift_to_Linux_Epoch_sec\": int  # the time shift in seconds to convert to Linux Epoch time (not specified in this case).\n",
    "\"data_checks\": dic[str]  #a dictionary that specifies various checks to perform on the data, such as the expected number of characters and delimiters.\n",
    "\"data_sizer_reader\": list[str]  # a dictionary that specifies how to read the sizer data, including the number of header rows, the starting and ending keywords for the diameter data, and a list of column headers.\n",
    "\"data_header\": list[str]  # a list of column headers for the data.\n",
    "\"time_column\": list[str]  #a list of column indices that specify the time data.\n",
    "\"time_format\": str  # the format of the time data.\n",
    "\"filename_regex\": str  # a regular expression that matches the filenames of the data files.\n",
    "\"base_interval_sec\": int  # the time interval in seconds for the data.\n",
    "\"data_delimiter\": str  # the delimiter used in the data file.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_settings = {\n",
    "    \"PASS3_data\": {\n",
    "        \"instrument_name\": \"PASS3\",\n",
    "        \"data_stream_name\": \"pass3\",\n",
    "        \"data_processing_function\": \"\",\n",
    "        \"data_loading_function\": \"general_load\",\n",
    "        \"relative_data_folder\": \"PASS3_data\",\n",
    "        \"skipRowsDict\": 0,\n",
    "        \"Time_shift_to_Linux_Epoch_sec\": 0,\n",
    "        \"data_checks\": {\n",
    "            \"characters\": [0,1100],\n",
    "            \"char_counts\": {\",\": 63, \":\": 2},\n",
    "            \"skip_rows\": 1,\n",
    "            \"skip_end\": 0\n",
    "        },\n",
    "        \"data_header\": [\n",
    "            \"Babs405nm[1/Mm]\",\n",
    "            \"Babs532nm[1/Mm]\",\n",
    "            \"Babs781nm[1/Mm]\",\n",
    "            \"Pressure[mb]\",\n",
    "            \"Temperature[C]\",\n",
    "            \"RH[%]\",\n",
    "            \"Bsca405nm[1/Mm]\",\n",
    "            \"Bsca532nm[1/Mm]\",\n",
    "            \"Bsca781nm[1/Mm]\",\n",
    "            \"BabsBGR405nm[1/Mm]\",\n",
    "            \"BabsBGR532nm[1/Mm]\",\n",
    "            \"BabsBGR781nm[1/Mm]\",\n",
    "            \"BscaBGR405nm[1/Mm]\",\n",
    "            \"BscaBGR532nm[1/Mm]\",\n",
    "            \"BscaBGR781nm[1/Mm]\",\n",
    "            \"Zero\"\n",
    "        ],\n",
    "        \"data_column\": [\n",
    "            5,\n",
    "            6,\n",
    "            7,\n",
    "            29,\n",
    "            30,\n",
    "            31,\n",
    "            33,\n",
    "            34,\n",
    "            35,\n",
    "            37,\n",
    "            38,\n",
    "            39,\n",
    "            57,\n",
    "            58,\n",
    "            59,\n",
    "            56\n",
    "        ],\n",
    "        \"time_column\": [0,1],\n",
    "        \"time_format\": \"%Y%m%d %H:%M:%S\",\n",
    "        \"filename_regex\": \"*BabsBGR_*.txt\",\n",
    "        \"base_interval_sec\": 2,\n",
    "        \"data_delimiter\": \",\"\n",
    "    },\n",
    "    \"CPC_3010_data\": {\n",
    "        \"instrument_name\": \"CPC_3010\",\n",
    "        \"data_stream_name\": \"cpc_3010\",\n",
    "        \"data_processing_function\": \"\",\n",
    "        \"data_loading_function\": \"general_load\",\n",
    "        \"relative_data_folder\": \"CPC_3010_data\",\n",
    "        \"skipRowsDict\": 0,\n",
    "        \"Time_shift_to_Linux_Epoch_sec\": 0,\n",
    "        \"data_checks\": {\n",
    "            \"characters\": [20,35],\n",
    "            \"char_counts\": {\",\": 4, \"/\": 0, \":\": 0},\n",
    "            \"skip_rows\": 1,\n",
    "            \"skip_end\": 0\n",
    "        },\n",
    "        \"data_header\": [\"CPC_count[#/sec]\", \"Temp_[C]\"],\n",
    "        \"data_column\": [1,2],\n",
    "        \"time_column\": 0,\n",
    "        \"time_format\": \"epoch\",\n",
    "        \"filename_regex\": \"CPC_3010*.csv\",\n",
    "        \"base_interval_sec\": 2,\n",
    "        \"data_delimiter\": \",\"\n",
    "    },\n",
    "    \"CAPS_dual_data\": {\n",
    "        \"instrument_name\": \"CAPS_dual\",\n",
    "        \"data_stream_name\": \"caps_dual\",\n",
    "        \"data_processing_function\": \"CAPS_turncation\",\n",
    "        \"data_loading_function\": \"general_load\",\n",
    "        \"relative_data_folder\": \"CAPS_dual_data\",\n",
    "        \"skipRowsDict\": 0,\n",
    "        \"Time_shift_to_Linux_Epoch_sec\": 0,\n",
    "        \"data_checks\": {\n",
    "            \"characters\": [200],\n",
    "            \"skip_rows\": 1,\n",
    "            \"skip_end\": 0,\n",
    "            \"char_counts\": {\",\": 42, \"/\": 2, \":\": 2},\n",
    "        },\n",
    "        \"data_header\": [\n",
    "            \"Bext_wet_CAPS_450nm[1/Mm]\",\n",
    "            \"Bsca_wet_CAPS_450nm[1/Mm]\",\n",
    "            \"Temp_wet_CAPS[K]\",\n",
    "            \"Bext_dry_CAPS_450nm[1/Mm]\",\n",
    "            \"Bsca_dry_CAPS_450nm[1/Mm]\",\n",
    "            \"Temp_dry_CAPS[K]\",\n",
    "            \"Wet_RH_preCAPS[%]\",\n",
    "            \"Wet_RH_postCAPS[%]\",\n",
    "            \"Humidifier_RH_CAPS[%]\",\n",
    "            \"dualCAPS_inlet_RH[%]\",\n",
    "            \"Wet_Temp_preCAPS[C]\",\n",
    "            \"Wet_Temp_postCAPS[C]\",\n",
    "            \"Humidifier_Temp[C]\",\n",
    "            \"dualCAPS_inlet_Temp[C]\",\n",
    "            \"Zero_dry_CAPS\",\n",
    "            \"Zero_wet_CAPS\"\n",
    "        ],\n",
    "        \"data_column\": [\n",
    "            2,\n",
    "            3,\n",
    "            6,\n",
    "            17,\n",
    "            18,\n",
    "            21,\n",
    "            32,\n",
    "            33,\n",
    "            34,\n",
    "            35,\n",
    "            36,\n",
    "            37,\n",
    "            38,\n",
    "            39,\n",
    "            41,\n",
    "            42\n",
    "        ],\n",
    "        \"time_column\": 0,\n",
    "        \"time_format\": \"%m/%d/%Y %I:%M:%S %p\",\n",
    "        \"filename_regex\": \"*.CAPS\",\n",
    "        \"base_interval_sec\": 2,\n",
    "        \"data_delimiter\": \",\"\n",
    "    },\n",
    "    \"SMPS_data\": {\n",
    "        \"instrument_name\": \"SMPS_data\",\n",
    "        \"data_stream_name\": [\"smps_1D\", \"smps_2D\"],\n",
    "        \"data_processing_function\": \"SMPS_processing\",\n",
    "        \"data_loading_function\": \"general_2d_sizer_load\",\n",
    "        \"relative_data_folder\": \"SMPS_data\",\n",
    "        \"skipRowsDict\": 0,\n",
    "        \"Time_shift_to_Linux_Epoch_sec\": 0,\n",
    "        \"data_checks\": {\n",
    "            \"characters\": [200],\n",
    "            \"skip_rows\": 24,\n",
    "            \"skip_end\": 0,\n",
    "            \"char_counts\": {\"/\": 2, \":\": 2},\n",
    "        },\n",
    "        \"data_sizer_reader\": {\n",
    "            \"header_rows\": 24,\n",
    "            \"Dp_start_keyword\": \"Diameter Midpoint (nm)\", \n",
    "            \"Dp_end_keyword\": \"Scan Time (s)\",\n",
    "            \"list_of_data_headers\": [\n",
    "                \"Lower Size (nm)\", \n",
    "                \"Upper Size (nm)\",\n",
    "                \"Sample Temp (C)\",\n",
    "                \"Sample Pressure (kPa)\",\n",
    "                \"Relative Humidity (%)\",\n",
    "                \"Median (nm)\",\n",
    "                \"Mean (nm)\",\n",
    "                \"Geo. Mean (nm)\",\n",
    "                \"Mode (nm)\",\n",
    "                \"Geo. Std. Dev.\",\n",
    "                \"Total Conc. (#/cm³)\"]\n",
    "            },\n",
    "        \"data_header\": [\n",
    "            \"Lower_Size_(nm)\",\n",
    "            \"Upper_Size_(nm)\",\n",
    "            \"Sample_Temp_(C)\",\n",
    "            \"Sample_Pressure_(kPa)\",\n",
    "            \"Relative_Humidity_(%)\",\n",
    "            \"Median_(nm)\",\n",
    "            \"Mean_(nm)\",\n",
    "            \"Geo_Mean_(nm)\",\n",
    "            \"Mode_(nm)\",\n",
    "            \"Geo_Std_Dev.\",\n",
    "            \"Total_Conc_(#/cc)\"\n",
    "        ],\n",
    "        \"time_column\": [1,2], \n",
    "        \"time_format\": \"%m/%d/%Y %H:%M:%S\",\n",
    "        \"filename_regex\": \"*.csv\",\n",
    "        \"base_interval_sec\": 90,\n",
    "        \"data_delimiter\": \",\"\n",
    "    },\n",
    "    \"APS_data\": {\n",
    "        \"instrument_name\": \"APS\",\n",
    "        \"data_stream_name\": [\"aps_1D\", \"aps_2D\"],\n",
    "        \"data_processing_function\": \"APS_processing\",\n",
    "        \"data_loading_function\": \"general_2d_sizer_load\",\n",
    "        \"relative_data_folder\": \"APS_data\",\n",
    "        \"skipRowsDict\": 0,\n",
    "        \"Time_shift_to_Linux_Epoch_sec\": 0,\n",
    "        \"data_checks\": {\n",
    "            \"characters\": [200],\n",
    "            \"skip_rows\": 6,\n",
    "            \"skip_end\": 0,\n",
    "            \"char_counts\": {\"/\": 4, \":\": 2},\n",
    "        },\n",
    "        \"data_sizer_reader\": {\n",
    "            \"header_rows\": 6,\n",
    "            \"Dp_start_keyword\": \"<0.523\", \n",
    "            \"Dp_end_keyword\": \"Event 1\",\n",
    "            \"list_of_data_headers\": [\n",
    "                \"<0.523\", \n",
    "                \"Total Flow\",\n",
    "                \"Box Temperature\",\n",
    "                \"Median(�m)\",\n",
    "                \"Mean(�m)\",\n",
    "                \"Geo. Mean(�m)\",\n",
    "                \"Mode(�m)\",\n",
    "                \"Geo. Std. Dev.\"]\n",
    "            },\n",
    "        \"data_header\": [\n",
    "            \"Lower_Size_(um)\",\n",
    "            \"Total_flow\",\n",
    "            \"Box_Temperature[C]\",\n",
    "            \"Median_(um)\",\n",
    "            \"Mean_(um)\",\n",
    "            \"Geo_Mean_(um)\",\n",
    "            \"Mode_(um)\",\n",
    "            \"Geo_Std_Dev\"],\n",
    "        \"time_column\": [1,2],\n",
    "        \"time_format\": \"%m/%d/%y %H:%M:%S\",\n",
    "        \"filename_regex\": \"*.txt\",\n",
    "        \"base_interval_sec\": 60,\n",
    "        \"data_delimiter\": \",\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise DataLake\n",
    "\n",
    "Initialise the DataLake object. We need to provide, a settings dictionary, and the path to the data.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "    Initializes the DataLake object.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        settings (dict): A dictionary of settings for each datastream to\n",
    "            be added to the DataLake. The keys of the dictionary should be\n",
    "            the names of the datastreams, and the values should be\n",
    "            dictionaries containing the settings for each datastream.\n",
    "        path (str, optional): The path to the data to be read in.\n",
    "            Default is None.\n",
    "        utc_to_local (int, optional): The UTC to local time offset\n",
    "            in hours. Default is 0. TODO: add UTC time zone support\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "data_path = os.path.join(path, 'data')\n",
    "\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data lake\n",
    "my_lake = DataLake(settings=lake_settings, path=data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Data\n",
    "\n",
    "Now that we are initialized, we can trigger the loading of the data. This will load the data into the DataLake object, and create a new DataStream object for each datastream in the settings dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lake.update_datastream()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can plot\n",
    "\n",
    "With the data loaded we can average it and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lake.reaverage_datastreams(average_base_sec=600)\n",
    "my_lake.list_datastream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    my_lake.datastreams['caps_dual'].return_time(datetime64=True),\n",
    "    my_lake.datastreams['caps_dual'].return_data(keys=['Bext_wet_CAPS_450nm[1/Mm]'])[0],\n",
    "    label='Extinction wet'\n",
    ")\n",
    "ax.plot(\n",
    "    my_lake.datastreams['caps_dual'].return_time(datetime64=True),\n",
    "    my_lake.datastreams['caps_dual'].return_data(keys=['Bext_dry_CAPS_450nm[1/Mm]'])[0],\n",
    "    label='Extinction dry'\n",
    ")\n",
    "\n",
    "plt.tick_params(rotation=-70)\n",
    "ax.set_ylabel('CAPS ext [1/Mm]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d, %H:%M'))\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizer Data timeseries\n",
    "\n",
    "If you forgot the header options in the data stream, you can always get the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1D time series headers')\n",
    "my_lake.datastreams['smps_1D'].return_header_dict()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1d sizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    my_lake.datastreams['smps_1D'].return_time(datetime64=True),\n",
    "    my_lake.datastreams['smps_1D'].return_data(keys=['Total_Conc_(#/cc)'])[0],\n",
    "    label='SMPS'\n",
    ")\n",
    "ax.plot(\n",
    "    my_lake.datastreams['cpc_3010'].return_time(datetime64=True),\n",
    "    my_lake.datastreams['cpc_3010'].return_data(keys=['CPC_count[#/sec]'])[0],\n",
    "    label='CPC counts'\n",
    ")\n",
    "plt.tick_params(rotation=-70, axis='x')\n",
    "ax.set_ylabel('Number Conc')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d, %H:%M'))\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2d sizer data\n",
    "\n",
    "We can pull the whole 2d sizer data and plot it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = my_lake.datastreams['smps_2D'].return_header_list()\n",
    "dp = np.array(dp, dtype=float)\n",
    "\n",
    "concentration = my_lake.datastreams['smps_2D'].return_data()\n",
    "\n",
    "print(f\"dp shape: {dp.shape} and concentration shape: {concentration.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shapes you can see the first dimension is the dp, and the second dimension is the time. We can plot this as a 2d image using contourf.\n",
    "\n",
    "We will first scale the concentration to a log scale, and crop the data heights to see the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration_log10 = np.clip(np.log10(concentration), a_min=0, a_max=5)  # clip to resolve banna curves.\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "plt.contourf(\n",
    "    my_lake.datastreams['smps_2D'].return_time(datetime64=True),\n",
    "    dp,\n",
    "    concentration_log10,\n",
    "    cmap=plt.cm.PuBu, levels=50)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Dp (nm)')\n",
    "plt.tick_params(rotation=-70, axis='x')\n",
    "plt.colorbar(label='Concentration [log10 #/cm3]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d, %H:%M'))\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot APS data\n",
    "\n",
    "We can make a similar plot for the APS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_aps = my_lake.datastreams['aps_2D'].return_header_list()\n",
    "dp_aps = np.array(dp_aps, dtype=float)\n",
    "concentration_aps = my_lake.datastreams['aps_2D'].return_data()\n",
    "\n",
    "concentration_aps_log10 = np.clip(np.log10(concentration_aps), a_min=0, a_max=4)  # clip to resolve banna curves.\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "plt.contourf(\n",
    "    my_lake.datastreams['aps_2D'].return_time(datetime64=True),\n",
    "    dp_aps,\n",
    "    concentration_aps_log10,\n",
    "    cmap=plt.cm.PuBu, levels=50)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Dp (microns)')\n",
    "plt.tick_params(rotation=-70, axis='x')\n",
    "plt.colorbar(label='Concentration [log10 #/cm3]')\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%d, %H:%M'))\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacula_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
